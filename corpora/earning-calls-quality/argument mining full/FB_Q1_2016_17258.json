{"id": 17258, "annotations": [{"id": 17361, "completed_by": 2, "result": [{"type": "labels", "value": {"end": 9, "text": "Operator", "start": 0, "labels": ["OPERATOR"]}, "origin": "manual", "to_name": "text", "from_name": "label-intro"}, {"type": "labels", "value": {"end": 93, "text": ": The next question is from Colin Sebastian with Robert W. Baird. Your line is open.", "start": 10, "labels": ["INTRO"]}, "origin": "manual", "to_name": "text", "from_name": "label-intro"}, {"type": "labels", "value": {"end": 113, "text": "Benjamin C. Gaither", "start": 93, "labels": ["ANALYST"]}, "origin": "manual", "to_name": "text", "from_name": "label-intro"}, {"type": "labels", "value": {"end": 501, "text": ": Hi, this is actually Ben on for Colin. This one is for Mark. You had already talked about the AI and machine learning applications with respect to Messenger and bots, but I was wondering what are some of the more nascent initiatives where you're applying machine learning, or maybe where we might see those investments manifest themselves in the user experience over the next few years?", "start": 114, "labels": ["QUESTION"]}, "origin": "manual", "to_name": "text", "from_name": "label-intro"}, {"type": "labels", "value": {"end": 524, "text": "Mark Elliot Zuckerberg", "start": 501, "labels": ["REPRESENTATIVE"]}, "origin": "manual", "to_name": "text", "from_name": "label-intro"}, {"type": "labels", "value": {"end": 2800, "text": ": So the biggest thing that we're focused on with artificial intelligence is building computer services that have better perception than people, so the basic human senses like seeing, hearing, language, core things that we do. I think it's possible to get to the point in the next five to 10 years where we have computer systems that are better than people at each of those things. That doesn't mean that the computers will be thinking or be generally better, but that is useful for a number of things. So for example, I talked about earlier, we are building this Moments app, so that way you can take photos on your phone. And if you use this app, our face recognition can look at the photos that you take and suggest that you might want to share photos that you took with a friend in them with that person. So that way, all the photos that might be of you and your friends' camera rolls, they can share with you. Another example is just spam filtering and just making sure that we can actually read the content and understand what's interesting to you or not and not show that. One obvious thing I think over time is if you just look at the way that we rank News Feed, today we use some basic signals like who you're friends with and what pages you like as some of the most important things for figuring out what \u2013 out of all of the millions and millions of pieces of content that are on Facebook, what we're going to show and what are going to be the most interesting things to you. That's because today our systems can't actually understand what the content means. We don't actually look at the photo and deeply understand what's in it or look at the videos and understand what's in it or read the links that people share and understand what's in them, but in the future we'll be able to, I think in a five or 10-year period. So all of these millions and millions of pieces content that are out there, whether or not you've added someone as a friend or have liked a page, we'll be able to know a lot better what types of things are going to be interesting to you to produce a much better feed of content. So that's just a basic example of where having human-level perception broadly is going to yield better experiences in a lot of the things that people care about today.", "start": 525, "labels": ["ANSWER"]}, "origin": "manual", "to_name": "text", "from_name": "label-intro"}], "was_cancelled": false, "ground_truth": true, "created_at": "2022-02-04T10:03:51.600114Z", "updated_at": "2022-02-04T10:03:51.600120Z", "lead_time": null, "prediction": {}, "result_count": 0, "task": 17258, "parent_prediction": null, "parent_annotation": null}], "data": {"year": 2016, "company": "FB", "my_text": "\nOperator: The next question is from Colin Sebastian with Robert W. Baird. Your line is open.\nBenjamin C. Gaither: Hi, this is actually Ben on for Colin. This one is for Mark. You had already talked about the AI and machine learning applications with respect to Messenger and bots, but I was wondering what are some of the more nascent initiatives where you're applying machine learning, or maybe where we might see those investments manifest themselves in the user experience over the next few years?\nMark Elliot Zuckerberg: So the biggest thing that we're focused on with artificial intelligence is building computer services that have better perception than people, so the basic human senses like seeing, hearing, language, core things that we do. I think it's possible to get to the point in the next five to 10 years where we have computer systems that are better than people at each of those things. That doesn't mean that the computers will be thinking or be generally better, but that is useful for a number of things. So for example, I talked about earlier, we are building this Moments app, so that way you can take photos on your phone. And if you use this app, our face recognition can look at the photos that you take and suggest that you might want to share photos that you took with a friend in them with that person. So that way, all the photos that might be of you and your friends' camera rolls, they can share with you. Another example is just spam filtering and just making sure that we can actually read the content and understand what's interesting to you or not and not show that. One obvious thing I think over time is if you just look at the way that we rank News Feed, today we use some basic signals like who you're friends with and what pages you like as some of the most important things for figuring out what \u2013 out of all of the millions and millions of pieces of content that are on Facebook, what we're going to show and what are going to be the most interesting things to you. That's because today our systems can't actually understand what the content means. We don't actually look at the photo and deeply understand what's in it or look at the videos and understand what's in it or read the links that people share and understand what's in them, but in the future we'll be able to, I think in a five or 10-year period. So all of these millions and millions of pieces content that are out there, whether or not you've added someone as a friend or have liked a page, we'll be able to know a lot better what types of things are going to be interesting to you to produce a much better feed of content. So that's just a basic example of where having human-level perception broadly is going to yield better experiences in a lot of the things that people care about today.", "quarter": 1}}